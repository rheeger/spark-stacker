# ğŸ§ª Spark Stacker Testing Suite

> **Complete Guide to Testing Framework & Methodology**

A comprehensive testing framework for the Spark Stacker trading system, featuring real market data integration, automated report generation, and systematic indicator validation.

---

## ğŸ“‹ Table of Contents

- [ğŸ§ª Spark Stacker Testing Suite](#-spark-stacker-testing-suite)
  - [ğŸ“‹ Table of Contents](#-table-of-contents)
  - [ğŸ¯ Overview \& Philosophy](#-overview--philosophy)
    - [Core Principles](#core-principles)
    - [Testing Paradigm](#testing-paradigm)
  - [ğŸ“ Directory Structure](#-directory-structure)
    - [Core Directories](#core-directories)
    - [Directory Details](#directory-details)
  - [ğŸ” Testing Categories \& Coverage Matrix](#-testing-categories--coverage-matrix)
    - [Current Coverage Status](#current-coverage-status)
    - [Future Coverage Needs](#future-coverage-needs)
  - [ğŸš€ Getting Started](#-getting-started)
    - [Prerequisites](#prerequisites)
    - [Environment Setup](#environment-setup)
    - [Quick Test Commands](#quick-test-commands)
  - [âš¡ CLI Usage](#-cli-usage)
    - [Available Commands](#available-commands)
    - [Command Examples](#command-examples)
      - [ğŸ¯ Run Indicator Demo with Synthetic Data](#-run-indicator-demo-with-synthetic-data)
      - [ğŸ“ˆ Run Backtest with Real Market Data](#-run-backtest-with-real-market-data)
      - [ğŸ”§ Custom Backtest Configuration](#-custom-backtest-configuration)
      - [ğŸ“‹ List Available Indicators](#-list-available-indicators)
  - [ğŸ“Š Report Generation](#-report-generation)
    - [Automated Report Generation](#automated-report-generation)
    - [Manual Report Generation](#manual-report-generation)
      - [ğŸ“Š Generate Report from JSON Results](#-generate-report-from-json-results)
      - [ğŸ“ˆ Generate Comparison Report](#-generate-comparison-report)
    - [Report Types](#report-types)
      - [ğŸ“‹ **Single Indicator Report**](#-single-indicator-report)
      - [ğŸ“Š **Comparison Report**](#-comparison-report)
      - [ğŸ¯ **Test Harness Report**](#-test-harness-report)
  - [âœ¨ Adding New Tests](#-adding-new-tests)
    - [Test Categories \& Placement](#test-categories--placement)
    - [Creating Unit Tests](#creating-unit-tests)
      - [ğŸ§ª **Basic Unit Test Template**](#-basic-unit-test-template)
    - [Creating Integration Tests](#creating-integration-tests)
      - [ğŸ”— **Integration Test Template**](#-integration-test-template)
    - [Adding Indicator Tests](#adding-indicator-tests)
      - [ğŸ“ˆ **New Indicator Checklist**](#-new-indicator-checklist)
      - [ğŸ”§ **Indicator Test Template**](#-indicator-test-template)
  - [ğŸ§© Fixtures \& Utilities](#-fixtures--utilities)
    - [Core Fixtures](#core-fixtures)
      - [ğŸ“Š **Data Fixtures** (from `conftest.py`)](#-data-fixtures-from-conftestpy)
      - [ğŸ—ï¸ **Environment Fixtures**](#ï¸-environment-fixtures)
    - [Data Generation](#data-generation)
      - [ğŸ² **Synthetic Data Factory** (`tests/_helpers/data_factory.py`)](#-synthetic-data-factory-tests_helpersdata_factorypy)
      - [ğŸ“ˆ **Available Patterns**](#-available-patterns)
    - [Helper Functions](#helper-functions)
      - [ğŸ”§ **Common Test Utilities**](#-common-test-utilities)
  - [ğŸ’¾ Data Management](#-data-management)
    - [Market Data Sources](#market-data-sources)
      - [ğŸ“Š **Data Source Priority**](#-data-source-priority)
      - [ğŸ”„ **Data Refresh Strategy**](#-data-refresh-strategy)
    - [Data Caching Strategy](#data-caching-strategy)
      - [ğŸ“ **Cache Locations**](#-cache-locations)
      - [âš™ï¸ **Cache Management Commands**](#ï¸-cache-management-commands)
    - [Synthetic Data Generation](#synthetic-data-generation)
      - [ğŸ² **Generation Parameters**](#-generation-parameters)
      - [ğŸ“Š **Data Quality Validation**](#-data-quality-validation)
  - [âœ… Best Practices](#-best-practices)
    - [Writing Quality Tests](#writing-quality-tests)
      - [ğŸ¯ **Test Structure (AAA Pattern)**](#-test-structure-aaa-pattern)
      - [ğŸ“ **Test Naming Convention**](#-test-naming-convention)
      - [ğŸ§ª **Parameterized Testing**](#-parameterized-testing)
    - [Performance Guidelines](#performance-guidelines)
      - [âš¡ **Speed Targets**](#-speed-targets)
      - [ğŸ·ï¸ **Test Markers**](#ï¸-test-markers)
      - [ğŸ¯ **Resource Management**](#-resource-management)
    - [Error Handling](#error-handling)
      - [ğŸš¨ **Exception Testing**](#-exception-testing)
      - [ğŸ” **Assertion Quality**](#-assertion-quality)
  - [ğŸ”§ Troubleshooting](#-troubleshooting)
    - [Common Issues](#common-issues)
      - [âŒ **Import Errors**](#-import-errors)
      - [âŒ **Missing Test Data**](#-missing-test-data)
      - [âŒ **Fixture Import Issues**](#-fixture-import-issues)
    - [Debug Strategies](#debug-strategies)
      - [ğŸ” **Verbose Testing**](#-verbose-testing)
      - [ğŸ“Š **Coverage Debugging**](#-coverage-debugging)
      - [ğŸ• **Performance Debugging**](#-performance-debugging)
    - [Environment Issues](#environment-issues)
      - [ğŸ **Python Version Problems**](#-python-version-problems)
      - [ğŸ’¾ **Disk Space Issues**](#-disk-space-issues)
      - [ğŸŒ **Network/API Issues**](#-networkapi-issues)
  - [ğŸ“š Additional Resources](#-additional-resources)

---

## ğŸ¯ Overview & Philosophy

This testing suite implements a **modern, systematic approach** to validating trading algorithms, indicators, and system components. Built from the ground up following the **Phase 3.5.1 refactor**, it emphasizes reliability, reproducibility, and comprehensive coverage.

### Core Principles

- **ğŸ¯ Reliability**: Consistent test results across environments using deterministic data
- **âš¡ Speed**: Fast feedback loops with quick test targets (`<3 minutes`)
- **ğŸ”¬ Isolation**: Clean separation between unit, integration, and system tests
- **ğŸ“Š Visibility**: Rich reporting with HTML visualizations and performance metrics
- **ğŸ—ï¸ Maintainability**: Centralized fixtures and utilities to minimize duplication
- **ğŸš€ Scalability**: Easy onboarding of new indicators and test scenarios

### Testing Paradigm

```mermaid
graph TD
    A[Real Market Data] --> B[Cached Test Data]
    C[Synthetic Data Factory] --> B
    B --> D[Test Fixtures]
    D --> E[Unit Tests]
    D --> F[Integration Tests]
    D --> G[Simulation Tests]
    E --> H[Coverage Reports]
    F --> H
    G --> H
    F --> I[HTML Reports]
    G --> I
```

---

## ğŸ“ Directory Structure

### Core Directories

```
tests/
â”œâ”€â”€ ğŸ“ backtesting/          # Backtesting engine tests
â”‚   â”œâ”€â”€ ğŸ“ unit/            # Fast, isolated tests
â”‚   â”œâ”€â”€ ğŸ“ integration/     # End-to-end workflows
â”‚   â”œâ”€â”€ ğŸ“ simulation/      # Strategy validation tests
â”‚   â””â”€â”€ ğŸ“ regression/      # Prevent regressions
â”œâ”€â”€ ğŸ“ indicators/          # Indicator-specific tests
â”‚   â”œâ”€â”€ ğŸ“ unit/            # Individual indicator tests
â”‚   â””â”€â”€ ğŸ“„ test_harness.py  # Batch indicator validation
â”œâ”€â”€ ğŸ“ connectors/          # Exchange connector tests
â”‚   â”œâ”€â”€ ğŸ“ unit/            # Mock-based connector tests
â”‚   â””â”€â”€ ğŸ“ integration/     # Live API tests
â”œâ”€â”€ ğŸ“ _fixtures/           # Test data and mocks
â”œâ”€â”€ ğŸ“ _helpers/            # Test utilities and factories
â”œâ”€â”€ ğŸ“ _utils/              # CLI tools and scripts
â”œâ”€â”€ ğŸ“ __test_data__/       # Static test datasets
â”œâ”€â”€ ğŸ“ __test_results__/    # Generated outputs (gitignored)
â”œâ”€â”€ ğŸ“„ conftest.py          # Pytest fixtures
â”œâ”€â”€ ğŸ“„ conftest.pyi         # Type stubs for fixtures
â””â”€â”€ ğŸ“„ README.md            # This file
```

### Directory Details

| Directory                  | Purpose                  | Key Files                   | Notes                     |
| -------------------------- | ------------------------ | --------------------------- | ------------------------- |
| `backtesting/unit/`        | âœ… Fast engine tests     | `test_backtest_engine.py`   | Core logic validation     |
| `backtesting/integration/` | âœ… End-to-end workflows  | `test_optimization.py`      | Complete trading flows    |
| `backtesting/simulation/`  | âœ… Strategy validation   | `test_simulation_engine.py` | Market scenario testing   |
| `backtesting/regression/`  | âœ… Prevent regressions   | Auto-generated              | Historical comparison     |
| `indicators/unit/`         | âœ… Individual indicators | `test_*_indicator.py`       | Signal generation tests   |
| `indicators/`              | âœ… Batch validation      | `test_harness.py`           | All indicators together   |
| `connectors/unit/`         | âœ… Mock API tests        | `test_*_connector.py`       | Offline validation        |
| `connectors/integration/`  | âœ… Live API tests        | `test_*_integration.py`     | Real API validation       |
| `_fixtures/`               | ğŸ”§ Test data             | Market scenarios            | Reusable test datasets    |
| `_helpers/`                | ğŸ”§ Utilities             | `data_factory.py`           | Synthetic data generation |
| `_utils/`                  | ğŸ”§ CLI & Scripts         | `cli.py`, `run_tests.py`    | Developer tools           |

---

## ğŸ” Testing Categories & Coverage Matrix

### Current Coverage Status

| Component                    | Unit Tests | Integration | Simulation | Report Generation |   Status    |
| ---------------------------- | :--------: | :---------: | :--------: | :---------------: | :---------: |
| **Backtesting Engine**       |     âœ…     |     âœ…      |     âœ…     |        âœ…         | ğŸŸ¢ Complete |
| **Simulation Engine**        |     âœ…     |     âœ…      |     âœ…     |        âœ…         | ğŸŸ¢ Complete |
| **Indicator Factory**        |     âœ…     |     âœ…      |     âŒ     |        âœ…         |   ğŸŸ¡ Good   |
| **Risk Management**          |     âœ…     |     âœ…      |     âŒ     |        âŒ         |   ğŸŸ¡ Good   |
| **Trading Engine**           |     âœ…     |     âœ…      |     âŒ     |        âŒ         |   ğŸŸ¡ Good   |
| **Data Manager**             |     âœ…     |     âœ…      |     âŒ     |        âŒ         |   ğŸŸ¡ Good   |
| **Connectors (Hyperliquid)** |     âœ…     |     âœ…      |     âŒ     |        âŒ         |   ğŸŸ¡ Good   |
| **Connectors (Coinbase)**    |     âœ…     |     âŒ      |     âŒ     |        âŒ         | ğŸŸ  Partial  |
| **Webhook Server**           |     âœ…     |     âŒ      |     âŒ     |        âŒ         | ğŸŸ  Partial  |
| **CLI Interface**            |     âœ…     |     âœ…      |     âŒ     |        âŒ         |   ğŸŸ¡ Good   |
| **Report Generator**         |     âŒ     |     âœ…      |     âŒ     |        âœ…         |   ğŸŸ¡ Good   |

### Future Coverage Needs

|   Priority    | Component                    | Tests Needed                          | Estimated Effort |
| :-----------: | ---------------------------- | ------------------------------------- | :--------------: |
|  ğŸ”´ **High**  | Report Generator Unit Tests  | Template validation, chart generation |     2-3 days     |
|  ğŸ”´ **High**  | Webhook Integration Tests    | End-to-end signal processing          |     1-2 days     |
| ğŸŸ¡ **Medium** | Coinbase Integration Tests   | Live API validation                   |      1 day       |
| ğŸŸ¡ **Medium** | Strategy Simulation Tests    | Multi-timeframe scenarios             |     2-3 days     |
|  ğŸŸ¢ **Low**   | Performance Regression Tests | Historical benchmark comparison       |     1-2 days     |
|  ğŸŸ¢ **Low**   | Load Testing Suite           | High-frequency trading scenarios      |     3-5 days     |

---

## ğŸš€ Getting Started

### Prerequisites

âœ… **Python 3.11+** installed
âœ… **Virtual environment** activated at `packages/spark-app/.venv`
âœ… **Dependencies** installed via `pip install -r requirements.txt`
âœ… **Market data** cached (auto-refreshed by test runner)

### Environment Setup

```bash
# Navigate to the spark-app package
cd packages/spark-app

# Activate virtual environment
source .venv/bin/activate  # Unix/Mac
# OR
.venv\Scripts\activate     # Windows

# Install dependencies (if not already done)
pip install -r requirements.txt

# Verify setup
python -m pytest --version
```

### Quick Test Commands

```bash
# âš¡ Quick test run (< 3 minutes, recommended before commits)
make test-quick
# OR
.venv/bin/python -m pytest -m "not slow" --cov=app

# ğŸ¯ Run all tests with coverage
.venv/bin/python -m pytest --cov=app

# ğŸ“Š Generate HTML coverage report
.venv/bin/python -m pytest --cov=app --cov-report=html

# ğŸ§¹ Clean test artifacts
make clean-results

# ğŸ”„ Refresh market data cache
python tests/_utils/refresh_test_market_data.py
```

> **âš ï¸ Important for Contributors**: Always run `make test-quick` before pushing changes to ensure your code doesn't break existing functionality. This quick test suite completes in under 3 minutes and provides essential validation.

---

## âš¡ CLI Usage

The testing suite includes a comprehensive CLI interface for running backtests, generating reports, and managing the testing workflow.

### Available Commands

| Command           | Purpose                   | Quick Example                                                           |
| ----------------- | ------------------------- | ----------------------------------------------------------------------- |
| `demo`            | Run preset demos          | `python tests/_utils/cli.py demo MACD`                                  |
| `real-data`       | Use live market data      | `python tests/_utils/cli.py real-data RSI --days 30`                    |
| `backtest`        | Custom backtest           | `python tests/_utils/cli.py backtest --symbol BTC/USD --indicator MACD` |
| `demo-macd`       | MACD demo shortcut        | `python tests/_utils/cli.py demo-macd`                                  |
| `list-indicators` | Show available indicators | `python tests/_utils/cli.py list-indicators`                            |

### Command Examples

#### ğŸ¯ Run Indicator Demo with Synthetic Data

```bash
# Quick demo with MACD indicator
python tests/_utils/cli.py demo MACD

# Demo with different symbol and timeframe
python tests/_utils/cli.py demo RSI --symbol BTC-USD --timeframe 4h

# Specify custom output directory
python tests/_utils/cli.py demo BOLLINGER --output-dir ./my_results
```

#### ğŸ“ˆ Run Backtest with Real Market Data

```bash
# Fetch 10 days of real Hyperliquid data and run RSI backtest
python tests/_utils/cli.py real-data RSI --symbol ETH-USD --days 10

# Use different timeframe
python tests/_utils/cli.py real-data MACD --timeframe 15m --days 5

# Use mainnet instead of testnet
python tests/_utils/cli.py real-data SMA --symbol BTC-USD --no-testnet
```

#### ğŸ”§ Custom Backtest Configuration

```bash
# Run backtest with custom data file
python tests/_utils/cli.py backtest \
  --symbol ETH/USDT \
  --indicator MACD \
  --data-file ./data/eth_1h.csv \
  --start-date 2024-01-01 \
  --end-date 2024-03-01
```

#### ğŸ“‹ List Available Indicators

```bash
python tests/_utils/cli.py list-indicators
```

**Expected Output:**

```
Available Indicators:
1. RSI - Relative Strength Index
2. MACD - Moving Average Convergence Divergence
3. BOLLINGER - Bollinger Bands
4. SMA - Simple Moving Average
5. EMA - Exponential Moving Average
```

---

## ğŸ“Š Report Generation

The testing suite automatically generates comprehensive HTML reports with interactive charts and detailed performance metrics.

### Automated Report Generation

Reports are **automatically generated** when running:

- âœ… CLI demo commands (`python tests/_utils/cli.py demo MACD`)
- âœ… CLI real-data commands (`python tests/_utils/cli.py real-data RSI`)
- âœ… Integration tests with the `generate_report=True` parameter
- âœ… Indicator test harness (`tests/indicators/test_harness.py`)

### Manual Report Generation

#### ğŸ“Š Generate Report from JSON Results

```bash
# Generate report from backtest results
python app/backtesting/reporting/generate_report.py \
  --results ./path/to/results.json \
  --market-data ./path/to/market_data.csv \
  --output-dir ./reports
```

#### ğŸ“ˆ Generate Comparison Report

```bash
# Compare multiple indicators
python app/backtesting/reporting/generate_report.py \
  --results ./rsi_results.json,./macd_results.json,./sma_results.json \
  --comparison \
  --output-dir ./comparison_reports
```

### Report Types

#### ğŸ“‹ **Single Indicator Report**

- **Performance Metrics**: Win rate, profit factor, Sharpe ratio, max drawdown
- **Interactive Charts**: Price chart with signals, equity curve, drawdown chart
- **Trade Analysis**: Detailed trade list with entry/exit points
- **Risk Metrics**: Position sizing, exposure analysis

#### ğŸ“Š **Comparison Report**

- **Side-by-Side Metrics**: Compare performance across indicators
- **Market Condition Analysis**: Bull/bear/sideways performance breakdown
- **Risk-Return Scatter Plot**: Visualize risk-adjusted returns
- **Ranking Tables**: Sort by various performance criteria

#### ğŸ¯ **Test Harness Report**

- **Indicator Validation**: Signal generation tests across market scenarios
- **Coverage Matrix**: Test coverage for each indicator
- **Performance Heatmap**: Visual performance comparison
- **Failure Analysis**: Detailed error reporting for failed tests

---

## âœ¨ Adding New Tests

The testing framework is designed for easy extension. Follow these guidelines to add new tests effectively.

### Test Categories & Placement

```mermaid
flowchart TD
    A[New Test] --> B{What are you testing?}
    B -->|Single function/method| C[Unit Test]
    B -->|Multiple components| D[Integration Test]
    B -->|Trading strategy| E[Simulation Test]
    B -->|Prevent regression| F[Regression Test]

    C --> G[tests/*/unit/]
    D --> H[tests/*/integration/]
    E --> I[tests/backtesting/simulation/]
    F --> J[tests/backtesting/regression/]
```

### Creating Unit Tests

#### ğŸ§ª **Basic Unit Test Template**

```python
# tests/indicators/unit/test_my_indicator.py
import pytest
import pandas as pd
from app.indicators.my_indicator import MyIndicator

class TestMyIndicator:
    """Test suite for MyIndicator."""

    def test_calculate_with_valid_data(self, price_dataframe):
        """Test calculation with valid market data."""
        indicator = MyIndicator(period=14)
        result = indicator.calculate(price_dataframe)

        # Assertions
        assert isinstance(result, pd.DataFrame)
        assert len(result) == len(price_dataframe)
        assert 'my_indicator_value' in result.columns
        assert not result['my_indicator_value'].isna().all()

    def test_generate_signal_buy_condition(self, price_dataframe):
        """Test signal generation for buy conditions."""
        indicator = MyIndicator(period=14)
        data_with_indicator = indicator.calculate(price_dataframe)

        # Test specific condition
        test_row = data_with_indicator.iloc[-1:]
        signal = indicator.generate_signal(test_row)

        assert signal is not None
        assert signal.direction in ['BUY', 'SELL', 'NEUTRAL']

    @pytest.mark.parametrize("period", [5, 14, 21, 50])
    def test_different_periods(self, price_dataframe, period):
        """Test indicator with different period parameters."""
        indicator = MyIndicator(period=period)
        result = indicator.calculate(price_dataframe)

        # Period-specific assertions
        assert len(result) == len(price_dataframe)
        # Ensure first (period-1) values are NaN due to insufficient data
        assert result['my_indicator_value'].iloc[:period-1].isna().all()
```

### Creating Integration Tests

#### ğŸ”— **Integration Test Template**

```python
# tests/backtesting/integration/test_my_workflow.py
import pytest
from app.backtesting.backtest_engine import BacktestEngine
from app.indicators.indicator_factory import IndicatorFactory

class TestMyWorkflow:
    """Integration tests for complete workflow."""

    def test_end_to_end_backtest(self, backtest_env, results_dir):
        """Test complete backtest workflow."""
        # Setup
        engine, data_manager = backtest_env
        indicator = IndicatorFactory.create("MyIndicator")

        # Execute
        results = engine.run_backtest(indicator)

        # Verify
        assert results is not None
        assert len(results.trades) > 0
        assert results.final_balance > 0

        # Generate report
        from app.backtesting.reporting.generator import generate_indicator_report
        report_path = generate_indicator_report(
            indicator_results=results,
            charts={},
            output_dir=str(results_dir)
        )
        assert report_path.exists()

    @pytest.mark.slow
    def test_optimization_workflow(self, backtest_env):
        """Test parameter optimization workflow."""
        engine, _ = backtest_env

        # Define parameter space
        param_space = {
            'period': [10, 14, 20],
            'threshold': [0.3, 0.5, 0.7]
        }

        # Run optimization
        optimizer = GeneticOptimizer(engine)
        results = optimizer.optimize("MyIndicator", param_space)

        assert len(results) > 0
        assert 'best_params' in results
        assert 'best_score' in results
```

### Adding Indicator Tests

#### ğŸ“ˆ **New Indicator Checklist**

1. **âœ… Create indicator class** in `app/indicators/`
2. **âœ… Register with factory** in `IndicatorFactory.register_defaults()`
3. **âœ… Add unit tests** in `tests/indicators/unit/`
4. **âœ… Update test harness** (automatic discovery)
5. **âœ… Create demo script** or add to CLI
6. **âœ… Add configuration** in `app/indicators/configs/`

#### ğŸ”§ **Indicator Test Template**

```python
# tests/indicators/unit/test_my_new_indicator.py
import pytest
import pandas as pd
from app.indicators.my_new_indicator import MyNewIndicator

class TestMyNewIndicator:
    """Comprehensive test suite for MyNewIndicator."""

    @pytest.fixture
    def indicator(self):
        """Create indicator instance with default parameters."""
        return MyNewIndicator(
            short_period=12,
            long_period=26,
            signal_period=9
        )

    def test_initialization(self, indicator):
        """Test indicator initialization."""
        assert indicator.short_period == 12
        assert indicator.long_period == 26
        assert indicator.signal_period == 9
        assert indicator.name == "MyNewIndicator"

    def test_calculate_returns_expected_columns(self, indicator, price_dataframe):
        """Test that calculation returns expected columns."""
        result = indicator.calculate(price_dataframe)

        expected_columns = [
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'my_indicator_line', 'my_indicator_signal', 'my_indicator_histogram'
        ]
        for col in expected_columns:
            assert col in result.columns

    def test_signal_generation_logic(self, indicator, price_dataframe):
        """Test signal generation logic."""
        data_with_indicator = indicator.calculate(price_dataframe)

        # Test multiple scenarios
        for i in range(len(data_with_indicator)-10, len(data_with_indicator)):
            window = data_with_indicator.iloc[:i+1]
            signal = indicator.generate_signal(window)

            if signal:
                assert signal.direction in ['BUY', 'SELL', 'NEUTRAL']
                assert signal.strength >= 0.0
                assert signal.strength <= 1.0

    @pytest.mark.parametrize("market_scenario", [
        "bull_trend", "bear_trend", "sideways_market", "volatile_market"
    ])
    def test_market_scenarios(self, indicator, market_scenario, request):
        """Test indicator performance in different market scenarios."""
        # Use parameterized market data
        market_data = request.getfixturevalue(f"{market_scenario}_data")

        result = indicator.calculate(market_data)

        # Scenario-specific assertions
        assert not result.empty
        assert not result['my_indicator_line'].isna().all()

    def test_integration_with_backtest_engine(self, indicator, backtest_env):
        """Test integration with backtesting engine."""
        engine, _ = backtest_env

        # Run quick backtest
        results = engine.run_backtest(indicator)

        assert results is not None
        assert hasattr(results, 'trades')
        assert hasattr(results, 'metrics')
```

---

## ğŸ§© Fixtures & Utilities

The testing framework provides a comprehensive set of fixtures and utilities to minimize boilerplate and ensure consistency.

### Core Fixtures

#### ğŸ“Š **Data Fixtures** (from `conftest.py`)

```python
@pytest.fixture
def price_dataframe():
    """Standard OHLCV DataFrame with 100 candles of trending data."""
    # Returns: pd.DataFrame with columns [timestamp, open, high, low, close, volume]

@pytest.fixture
def temp_csv_dir():
    """Temporary directory with CSV data files."""
    # Returns: Path to temporary directory (auto-cleaned)

@pytest.fixture
def backtest_env():
    """Complete backtesting environment setup."""
    # Returns: (BacktestEngine, DataManager) tuple

@pytest.fixture
def results_dir():
    """Temporary directory for test output files."""
    # Returns: Path to temporary directory (auto-cleaned)
```

#### ğŸ—ï¸ **Environment Fixtures**

```python
@pytest.fixture
def mock_connector():
    """Mock exchange connector for offline testing."""
    # Returns: Mock connector with predefined responses

@pytest.fixture
def live_connector():
    """Live connector for integration testing."""
    # Returns: Real connector instance (testnet)

@pytest.fixture
def indicator_factory():
    """Indicator factory with all indicators registered."""
    # Returns: IndicatorFactory instance
```

### Data Generation

#### ğŸ² **Synthetic Data Factory** (`tests/_helpers/data_factory.py`)

```python
from tests._helpers.data_factory import make_price_dataframe

# Generate different market patterns
trending_data = make_price_dataframe(
    rows=100,
    pattern="trend",
    noise=0.02,
    seed=42
)

sideways_data = make_price_dataframe(
    rows=100,
    pattern="sideways",
    noise=0.01,
    seed=42
)

volatile_data = make_price_dataframe(
    rows=100,
    pattern="volatile",
    noise=0.05,
    seed=42
)
```

#### ğŸ“ˆ **Available Patterns**

| Pattern       | Description                    | Use Case              |
| ------------- | ------------------------------ | --------------------- |
| `trend`       | Consistent upward trend        | Bull market testing   |
| `downtrend`   | Consistent downward trend      | Bear market testing   |
| `sideways`    | Range-bound movement           | Consolidation testing |
| `volatile`    | High volatility with reversals | Stress testing        |
| `mean_revert` | Mean-reverting behavior        | Counter-trend testing |

### Helper Functions

#### ğŸ”§ **Common Test Utilities**

```python
from tests._helpers import test_utils

# Validate DataFrame structure
test_utils.validate_ohlcv_dataframe(df)

# Create mock trade objects
trades = test_utils.create_mock_trades(count=10)

# Generate test market scenarios
scenarios = test_utils.generate_market_scenarios()

# Compare performance metrics
comparison = test_utils.compare_indicator_performance(results1, results2)
```

---

## ğŸ’¾ Data Management

The testing framework uses a sophisticated data management strategy that balances realism, performance, and reliability.

### Market Data Sources

#### ğŸ“Š **Data Source Priority**

1. **ğŸ¥‡ Cached Real Data** - Fetched from live exchanges, cached locally
2. **ğŸ¥ˆ Test Fixtures** - Curated datasets for specific scenarios
3. **ğŸ¥‰ Synthetic Data** - Generated data for fallback scenarios

#### ğŸ”„ **Data Refresh Strategy**

```mermaid
graph LR
    A[Test Execution] --> B{Cache Age Check}
    B -->|< 24 hours| C[Use Cached Data]
    B -->|> 24 hours| D[Refresh from API]
    D --> E[Update Cache]
    E --> C
    C --> F[Run Tests]
```

### Data Caching Strategy

#### ğŸ“ **Cache Locations**

```
tests/
â”œâ”€â”€ __test_data__/
â”‚   â”œâ”€â”€ market_data/           # Real market data cache
â”‚   â”‚   â”œâ”€â”€ hyperliquid/      # Hyperliquid connector data
â”‚   â”‚   â”œâ”€â”€ coinbase/         # Coinbase connector data
â”‚   â”‚   â””â”€â”€ demo/             # Demo/synthetic data
â”‚   â””â”€â”€ market_scenarios/     # Curated test scenarios
â”‚       â”œâ”€â”€ bull_market.csv
â”‚       â”œâ”€â”€ bear_market.csv
â”‚       â”œâ”€â”€ sideways_market.csv
â”‚       â””â”€â”€ volatile_market.csv
```

#### âš™ï¸ **Cache Management Commands**

```bash
# Force refresh all cached data
python tests/_utils/refresh_test_market_data.py

# Check cache status
python tests/_utils/cli.py cache-status

# Clean old cache files
python tests/_utils/cli.py clean-cache --older-than 7d
```

### Synthetic Data Generation

#### ğŸ² **Generation Parameters**

```python
# Market pattern configurations
PATTERNS = {
    'trend': {
        'drift': 0.0005,      # Daily return bias
        'volatility': 0.02,   # Daily volatility
        'trend_strength': 0.8 # Trend consistency
    },
    'sideways': {
        'drift': 0.0,
        'volatility': 0.015,
        'range_bound': True
    },
    'volatile': {
        'drift': 0.0,
        'volatility': 0.05,
        'regime_changes': True
    }
}
```

#### ğŸ“Š **Data Quality Validation**

All generated data is validated for:

- âœ… **OHLC Consistency** - High â‰¥ Open/Close, Low â‰¤ Open/Close
- âœ… **Volume Realism** - Positive values with realistic distribution
- âœ… **Timestamp Ordering** - Monotonically increasing timestamps
- âœ… **Statistical Properties** - Returns distribution, volatility clustering

---

## âœ… Best Practices

### Writing Quality Tests

#### ğŸ¯ **Test Structure (AAA Pattern)**

```python
def test_feature_with_specific_condition():
    """Test description explaining what and why."""
    # ARRANGE - Set up test data and dependencies
    indicator = MyIndicator(period=14)
    data = create_test_data()

    # ACT - Execute the functionality being tested
    result = indicator.calculate(data)

    # ASSERT - Verify the expected outcomes
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(data)
    assert 'indicator_value' in result.columns
```

#### ğŸ“ **Test Naming Convention**

```python
# âœ… Good - Describes what is being tested and the expected outcome
def test_rsi_calculation_with_14_period_returns_normalized_values():

def test_macd_signal_generation_when_lines_cross_returns_buy_signal():

def test_backtest_engine_with_insufficient_data_raises_value_error():

# âŒ Bad - Vague or unclear purpose
def test_rsi():

def test_signals():

def test_error():
```

#### ğŸ§ª **Parameterized Testing**

```python
@pytest.mark.parametrize("period,expected_nan_count", [
    (5, 4),    # First 4 values should be NaN
    (14, 13),  # First 13 values should be NaN
    (21, 20),  # First 20 values should be NaN
])
def test_indicator_warmup_period(price_dataframe, period, expected_nan_count):
    """Test that indicator respects warmup period."""
    indicator = RSIIndicator(period=period)
    result = indicator.calculate(price_dataframe)

    nan_count = result['rsi'].isna().sum()
    assert nan_count == expected_nan_count
```

### Performance Guidelines

#### âš¡ **Speed Targets**

- **Unit Tests**: `< 50ms per test`
- **Integration Tests**: `< 5s per test`
- **Quick Test Suite**: `< 3 minutes total`
- **Full Test Suite**: `< 15 minutes total`

#### ğŸ·ï¸ **Test Markers**

```python
# Mark slow tests to exclude from quick runs
@pytest.mark.slow
def test_optimization_with_large_parameter_space():
    """Test that takes several minutes to complete."""
    pass

# Mark tests requiring external resources
@pytest.mark.integration
def test_live_api_connection():
    """Test requiring live API access."""
    pass

# Mark flaky tests for special handling
@pytest.mark.flaky(reruns=3)
def test_timing_sensitive_operation():
    """Test that occasionally fails due to timing."""
    pass
```

#### ğŸ¯ **Resource Management**

```python
def test_with_resource_cleanup():
    """Example of proper resource management."""
    # Use context managers for automatic cleanup
    with TemporaryDirectory() as temp_dir:
        # Use fixtures that auto-cleanup
        data_file = create_test_csv(temp_dir)

        # Test logic here
        result = process_file(data_file)

        # Assertions
        assert result is not None
    # temp_dir automatically cleaned up
```

### Error Handling

#### ğŸš¨ **Exception Testing**

```python
def test_indicator_with_invalid_period_raises_value_error():
    """Test that invalid parameters raise appropriate exceptions."""
    with pytest.raises(ValueError, match="Period must be positive"):
        RSIIndicator(period=-5)

def test_backtest_with_insufficient_data_logs_warning(caplog):
    """Test that warnings are logged for edge cases."""
    engine = BacktestEngine()
    small_dataset = create_minimal_dataset(rows=5)

    engine.run_backtest(small_dataset)

    assert "Insufficient data" in caplog.text
    assert caplog.records[0].levelname == "WARNING"
```

#### ğŸ” **Assertion Quality**

```python
# âœ… Good - Specific assertions with helpful messages
def test_trade_execution():
    trades = execute_strategy(data)

    assert len(trades) > 0, "Strategy should generate at least one trade"
    assert all(t.quantity > 0 for t in trades), "All trades should have positive quantity"
    assert trades[0].entry_price > 0, f"Entry price should be positive, got {trades[0].entry_price}"

# âŒ Bad - Vague assertions
def test_trade_execution():
    trades = execute_strategy(data)
    assert trades
    assert trades[0]
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### âŒ **Import Errors**

**Problem**: `ModuleNotFoundError: No module named 'app.indicators'`

**Solution**:

```bash
# Ensure you're in the correct directory
cd packages/spark-app

# Verify virtual environment is activated
source .venv/bin/activate

# Check Python path includes the current directory
python -c "import sys; print(sys.path)"

# Run tests with explicit path
PYTHONPATH=. python -m pytest tests/
```

#### âŒ **Missing Test Data**

**Problem**: `FileNotFoundError: Market data file not found`

**Solution**:

```bash
# Refresh market data cache
python tests/_utils/refresh_test_market_data.py

# Run tests with synthetic data fallback
python -m pytest --allow-synthetic-data

# Check data directory contents
ls -la tests/__test_data__/market_data/
```

#### âŒ **Fixture Import Issues**

**Problem**: `NameError: name 'price_dataframe' is not defined`

**Solution**:

```python
# Ensure conftest.py is in the right location
# tests/conftest.py should exist

# Check fixture definition
def test_my_function(price_dataframe):  # âœ… Correct parameter name
    # Test logic

# Verify pytest is discovering conftest.py
python -m pytest --fixtures | grep price_dataframe
```

### Debug Strategies

#### ğŸ” **Verbose Testing**

```bash
# Run with maximum verbosity
python -m pytest -vvv --tb=long tests/path/to/test.py

# Show local variables in tracebacks
python -m pytest --tb=long --showlocals

# Drop into debugger on failures
python -m pytest --pdb

# Run single test with debugging
python -m pytest -k "test_specific_function" -s --pdb
```

#### ğŸ“Š **Coverage Debugging**

```bash
# Generate coverage report with missing lines
python -m pytest --cov=app --cov-report=term-missing

# Generate detailed HTML coverage report
python -m pytest --cov=app --cov-report=html
open _htmlcov/index.html

# Coverage for specific module
python -m pytest --cov=app.indicators --cov-report=term
```

#### ğŸ• **Performance Debugging**

```bash
# Profile test execution time
python -m pytest --durations=10

# Profile with more detailed timing
python -m pytest --durations=0

# Run with profiling
python -m pytest --profile --profile-svg
```

### Environment Issues

#### ğŸ **Python Version Problems**

**Problem**: Tests pass locally but fail in CI

**Solution**:

```bash
# Check Python version consistency
python --version  # Should be 3.11+

# Verify package versions
pip freeze > requirements_current.txt
diff requirements.txt requirements_current.txt

# Recreate virtual environment if needed
rm -rf .venv
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

#### ğŸ’¾ **Disk Space Issues**

**Problem**: Tests fail with "No space left on device"

**Solution**:

```bash
# Clean test artifacts
make clean-results

# Clean Python cache
find . -type d -name "__pycache__" -exec rm -rf {} +
find . -name "*.pyc" -delete

# Clean old coverage files
rm -rf _htmlcov/ .coverage

# Check disk usage
du -sh tests/__test_results__/
```

#### ğŸŒ **Network/API Issues**

**Problem**: Integration tests fail with connection errors

**Solution**:

```bash
# Run offline tests only
python -m pytest -m "not integration"

# Use synthetic data
python -m pytest --allow-synthetic-data

# Check API connectivity
python -c "import requests; print(requests.get('https://api.hyperliquid.xyz/info').status_code)"

# Use testnet instead of mainnet
export USE_TESTNET=true
python -m pytest tests/connectors/
```

---

## ğŸ“š Additional Resources

- **ğŸ“– Pytest Documentation**: <https://docs.pytest.org/>
- **ğŸ”§ Coverage.py Documentation**: <https://coverage.readthedocs.io/>
- **ğŸ“Š Phase 3.5.1 Checklist**: `packages/shared/docs/checklists/phase3.5.1-indicator-testing-reporting.md`
- **ğŸ” Audit Report**: `packages/shared/docs/retros/5-17-backtesting-suite-audit.md`
- **ğŸ—ï¸ Architecture Guide**: `packages/spark-app/README.md`

---

**ğŸ‰ Happy Testing!**

_For questions or issues, please refer to the troubleshooting section above or check the project documentation._
