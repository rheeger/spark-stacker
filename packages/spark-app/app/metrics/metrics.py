"""
Metrics collection for the Spark Stacker trading application.

This module defines all the metrics that are collected and exposed to Prometheus.
"""

import logging
import os
import time

from prometheus_client import (REGISTRY, CollectorRegistry, Counter, Gauge,
                               Histogram)

# Create a custom registry for our metrics
custom_registry = CollectorRegistry()

# Global flag for metrics registration
_METRICS_REGISTERED = False

def _register_metrics():
    """Register all metrics if they haven't been registered yet."""
    global _METRICS_REGISTERED, _START_TIME, uptime_seconds, trades_total, active_positions
    global signal_count, api_requests_total, api_latency_seconds, order_execution_seconds
    global rate_limit_remaining, margin_ratio, liquidation_price, capital_utilization_percent
    global max_drawdown_percent, pnl_percent, candle_data, macd_values, macd_signal_state
    global mvp_signal_latency, mvp_trades, mvp_position_size, mvp_pnl

    if _METRICS_REGISTERED:
        return

    # Application start time for uptime calculation
    _START_TIME = time.time()

    # System uptime and health metrics
    uptime_seconds = Gauge(
        "spark_stacker_uptime_seconds",
        "Application uptime in seconds",
        registry=custom_registry
    )

    # Trade metrics
    trades_total = Counter(
        "spark_stacker_trades_total",
        "Total number of trades executed",
        ["result", "exchange", "side"],
        registry=custom_registry
    )

    # Position metrics
    active_positions = Gauge(
        "spark_stacker_active_positions",
        "Number of currently active positions",
        ["exchange", "market", "side"],
        registry=custom_registry
    )

    # Signal metrics
    signal_count = Counter(
        "spark_stacker_signal_count",
        "Number of signals generated by indicators",
        ["indicator", "signal"],
        registry=custom_registry
    )

    # API metrics
    api_requests_total = Counter(
        "spark_stacker_api_requests_total",
        "Total number of API requests made",
        ["exchange", "endpoint", "method"],
        registry=custom_registry
    )

    api_latency_seconds = Histogram(
        "spark_stacker_api_latency_seconds",
        "API request latency in seconds",
        ["exchange", "endpoint"],
        buckets=(
            0.001,
            0.005,
            0.01,
            0.025,
            0.05,
            0.075,
            0.1,
            0.25,
            0.5,
            0.75,
            1.0,
            2.5,
            5.0,
            7.5,
            10.0,
            float("inf"),
        ),
        registry=custom_registry
    )

    order_execution_seconds = Histogram(
        "spark_stacker_order_execution_seconds",
        "Order execution time in seconds",
        ["exchange", "order_type"],
        buckets=(0.01, 0.05, 0.1, 0.5, 1.0, 2.5, 5.0, 10.0, float("inf")),
        registry=custom_registry
    )

    rate_limit_remaining = Gauge(
        "spark_stacker_rate_limit_remaining",
        "Number of API requests remaining before rate limit",
        ["exchange", "endpoint"],
        registry=custom_registry
    )

    # Risk metrics
    margin_ratio = Gauge(
        "spark_stacker_margin_ratio",
        "Current margin ratio for positions",
        ["exchange", "position_id"],
        registry=custom_registry
    )

    liquidation_price = Gauge(
        "spark_stacker_liquidation_price",
        "Liquidation price for positions",
        ["exchange", "position_id"],
        registry=custom_registry
    )

    capital_utilization_percent = Gauge(
        "spark_stacker_capital_utilization_percent",
        "Percentage of capital currently in use",
        registry=custom_registry
    )

    max_drawdown_percent = Gauge(
        "spark_stacker_max_drawdown_percent",
        "Maximum drawdown percentage",
        ["timeframe"],
        registry=custom_registry
    )

    # Performance metrics
    pnl_percent = Gauge(
        "spark_stacker_pnl_percent",
        "Profit and loss percentage",
        ["strategy", "position_type"],
        registry=custom_registry
    )

    # Candle data metrics
    candle_data = Gauge(
        "spark_stacker_candle_data",
        "Latest candle data values",
        ["market", "timeframe", "field"],
        registry=custom_registry
    )

    # Historical candle data metrics with timestamps
    historical_candle_data = Gauge(
        "spark_stacker_historical_candle",
        "Historical candle data with timestamps",
        ["market", "timeframe", "field", "timestamp"],
        registry=custom_registry
    )

    # MACD indicator metrics
    macd_values = Gauge(
        "spark_stacker_macd_values",
        "MACD indicator values",
        ["market", "timeframe", "component"],
        registry=custom_registry
    )

    macd_signal_state = Gauge(
        "spark_stacker_macd_signal_state",
        "Current MACD signal state",
        ["market", "timeframe"],
        registry=custom_registry
    )

    # MVP Strategy Metrics
    mvp_signal_latency = Histogram(
        "spark_stacker_mvp_signal_latency_ms",
        "Signal processing latency in milliseconds for MVP strategy",
        buckets=(1, 5, 10, 25, 50, 100, 250, 500, 1000, float("inf")),
        registry=custom_registry
    )

    mvp_trades = Counter(
        "spark_stacker_mvp_trades_total",
        "Total number of MVP strategy trades",
        ["success", "side", "position_type"],
        registry=custom_registry
    )

    mvp_position_size = Gauge(
        "spark_stacker_mvp_position_size",
        "Current position size for MVP strategy",
        ["position_type", "side"],
        registry=custom_registry
    )

    mvp_pnl = Gauge(
        "spark_stacker_mvp_pnl",
        "PnL for MVP strategy",
        ["position_type"],
        registry=custom_registry
    )

    _METRICS_REGISTERED = True

def clear_metrics():
    """Clear all metrics from the registry. Useful for testing."""
    global _METRICS_REGISTERED
    _METRICS_REGISTERED = False

    # Get a list of all collectors
    collectors = list(custom_registry._collector_to_names.keys())

    # Unregister each collector
    for collector in collectors:
        custom_registry.unregister(collector)

# Register metrics on module import
_register_metrics()

# Helper functions
def _update_uptime():
    """Update the uptime metric."""
    try:
        uptime_seconds.set(time.time() - _START_TIME)
    except (NameError, AttributeError):
        pass  # Metric likely not registered


def record_trade(result: str, exchange: str, side: str) -> None:
    """
    Record a trade execution.

    Args:
        result: 'success' or 'failure'
        exchange: The exchange name
        side: 'buy' or 'sell'
    """
    try:
        trades_total.labels(result=result, exchange=exchange, side=side).inc()
    except NameError:
        pass # Metric likely not registered
    _update_uptime()


def update_position(exchange: str, market: str, side: str, count: int) -> None:
    """
    Update the count of active positions.

    Args:
        exchange: The exchange name
        market: The market symbol
        side: 'long' or 'short'
        count: The current count of positions
    """
    try:
        active_positions.labels(exchange=exchange, market=market, side=side).set(count)
    except NameError:
        pass # Metric likely not registered
    _update_uptime()


def record_signal(indicator: str, signal: str) -> None:
    """
    Record a signal generated by an indicator.

    Args:
        indicator: The name of the indicator
        signal: 'buy', 'sell', or 'neutral'
    """
    try:
        signal_count.labels(indicator=indicator, signal=signal).inc()
    except NameError:
        pass # Metric likely not registered
    _update_uptime()


def record_api_request(exchange: str, endpoint: str, method: str) -> None:
    """
    Record an API request.

    Args:
        exchange: The exchange name
        endpoint: The API endpoint path
        method: The HTTP method ('GET', 'POST', etc.)
    """
    try:
        api_requests_total.labels(exchange=exchange, endpoint=endpoint, method=method).inc()
    except NameError:
        pass # Metric likely not registered
    _update_uptime()


def observe_api_latency(exchange: str, endpoint: str, seconds: float) -> None:
    """
    Record the latency of an API request.

    Args:
        exchange: The exchange name
        endpoint: The API endpoint path
        seconds: The request duration in seconds
    """
    try:
        api_latency_seconds.labels(exchange=exchange, endpoint=endpoint).observe(seconds)
    except NameError:
        pass # Metric likely not registered
    _update_uptime()

def record_mvp_signal_latency(latency_ms: float) -> None:
    """
    Record signal processing latency for MVP strategy.

    Args:
        latency_ms: Latency in milliseconds
    """
    try:
        mvp_signal_latency.observe(latency_ms)
    except NameError:
        pass  # Metric likely not registered
    _update_uptime()

def record_mvp_trade(success: bool, side: str, position_type: str) -> None:
    """
    Record a trade execution for MVP strategy.

    Args:
        success: Whether the trade was successful
        side: Trade side ('buy' or 'sell')
        position_type: Type of position ('main' or 'hedge')
    """
    try:
        mvp_trades.labels(
            success="success" if success else "failure",
            side=side,
            position_type=position_type
        ).inc()
    except NameError:
        pass  # Metric likely not registered
    _update_uptime()

def update_mvp_position_size(position_type: str, side: str, size: float) -> None:
    """
    Update position size for MVP strategy.

    Args:
        position_type: Type of position ('main' or 'hedge')
        side: Position side ('buy' or 'sell')
        size: Position size
    """
    try:
        mvp_position_size.labels(position_type=position_type, side=side).set(size)
    except NameError:
        pass  # Metric likely not registered
    _update_uptime()

def update_mvp_pnl(position_type: str, pnl: float) -> None:
    """
    Update PnL for MVP strategy.

    Args:
        position_type: Type of position ('main' or 'hedge')
        pnl: Profit and loss value
    """
    try:
        mvp_pnl.labels(position_type=position_type).set(pnl)
    except NameError:
        pass  # Metric likely not registered
    _update_uptime()

def update_candle_data(market: str, timeframe: str, field: str, value: float, timestamp: int = None) -> None:
    """
    Update candle data metrics.

    Args:
        market: Market symbol (e.g., 'ETH-USD')
        timeframe: Time interval (e.g., '1m', '5m', '1h')
        field: Candle field ('open', 'high', 'low', 'close', 'volume')
        value: Field value
        timestamp: Optional timestamp for historical data (milliseconds since epoch)
    """
    logger = logging.getLogger(__name__)
    # Timestamps are used for logging but not stored differently in Prometheus
    # Prometheus will use its own scrape time
    actual_ts = timestamp if timestamp else int(time.time() * 1000)

    logger.debug(f"Setting candle data metric: market={market}, timeframe={timeframe}, field={field}, value={value}, timestamp={actual_ts}")

    try:
        # Set the same metric regardless of whether it's historical or real-time
        # Prometheus treats all updates the same - it will use its own scrape time
        labels = {"market": market, "timeframe": timeframe, "field": field}
        candle_data.labels(**labels).set(value)

        # Log based on whether this is historical or real-time data
        if timestamp:
            logger.debug(f"Published historical candle data for {market}/{timeframe}/{field}: {value} @ {actual_ts}")
        else:
            logger.debug(f"Published real-time candle data for {market}/{timeframe}/{field}: {value}")
    except NameError as ne:
        logger.error(f"Failed to set candle data: NameError - {ne}")
    except Exception as e:
        logger.error(f"Failed to set candle data: {e}", exc_info=True)

    _update_uptime()

def update_macd_indicator(market: str, timeframe: str, component: str, value: float, timestamp: int = None) -> None:
    """
    Update MACD indicator metrics.

    Args:
        market: Market symbol (e.g., 'ETH-USD')
        timeframe: Time interval (e.g., '1m', '5m', '1h')
        component: Component name ('macd_line', 'signal_line', 'histogram')
        value: Component value
        timestamp: Optional timestamp for historical data (milliseconds since epoch)
    """
    logger = logging.getLogger(__name__)
    # Timestamps are used for logging but not stored differently in Prometheus
    # Prometheus will use its own scrape time
    actual_ts = timestamp if timestamp else int(time.time() * 1000)

    logger.debug(f"Setting MACD metric: market={market}, timeframe={timeframe}, component={component}, value={value}, timestamp={actual_ts}")

    try:
        # Set the same metric regardless of whether it's historical or real-time
        # Prometheus treats all updates the same - it will use its own scrape time
        labels = {"market": market, "timeframe": timeframe, "component": component}
        macd_values.labels(**labels).set(value)

        # Log based on whether this is historical or real-time data
        if timestamp:
            logger.debug(f"Published historical MACD data for {market}/{timeframe}/{component}: {value} @ {actual_ts}")
        else:
            logger.debug(f"Published real-time MACD data for {market}/{timeframe}/{component}: {value}")
    except NameError as ne:
        logger.error(f"Failed to update MACD indicator: NameError - {ne}")
    except Exception as e:
        logger.error(f"Failed to update MACD indicator: {e}", exc_info=True)

    _update_uptime()

def update_mvp_signal_state(market: str, timeframe: str, state: int) -> None:
    """
    Update MACD signal state.

    Args:
        market: Market symbol (e.g., 'ETH-USD')
        timeframe: Time interval (e.g., '1m', '5m', '1h')
        state: Signal state (1 for bullish, -1 for bearish, 0 for neutral)
    """
    try:
        macd_signal_state.labels(market=market, timeframe=timeframe).set(state)
    except NameError:
        pass  # Metric likely not registered
    _update_uptime()

# Historical metrics track function for Prometheus
def _create_historical_metrics_tracker():
    """Create a function to track historical metrics with timestamps for Prometheus"""
    import time

    from prometheus_client import Info

    # Store last timestamp for each metric type to avoid duplicates
    last_timestamps = {}

    def _set_metric_with_timestamp(metric, labels, value, timestamp=None):
        """Set a metric value with optional timestamp"""
        logger = logging.getLogger(__name__)

        try:
            if timestamp is None:
                # Use current time if no timestamp provided
                metric.labels(**labels).set(value)
                logger.debug(f"Set real-time metric {metric._name} with labels {labels}")
            else:
                # Create key for this specific metric + labels
                ts_key = f"{metric._name}:{str(labels)}"

                # Ensure we don't publish duplicate timestamps which can cause issues
                if ts_key in last_timestamps and timestamp <= last_timestamps[ts_key]:
                    timestamp = last_timestamps[ts_key] + 1000  # Add 1 second

                # Store the timestamp
                last_timestamps[ts_key] = timestamp

                # Use the same metric for historical data
                # Just set the metric like normal - don't create specialized historical gauges
                metric.labels(**labels).set(value)

                logger.info(f"Set historical {metric._name} with labels {labels} for timestamp {timestamp}")
        except Exception as e:
            logger.error(f"Error setting metric with timestamp: {e}")
            # Fallback: just set the value without timestamp
            try:
                metric.labels(**labels).set(value)
            except Exception as fallback_e:
                logger.error(f"Error in fallback metric setting: {fallback_e}")

    return _set_metric_with_timestamp

# Create the historical metrics tracker
set_metric_with_timestamp = _create_historical_metrics_tracker()
